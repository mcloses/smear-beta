{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "\n",
    "from src.utils.config import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_config, local_config = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used to run: cuda:0\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"../yolov7/\")\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device used to run: {device}\")\n",
    "weigths = torch.load(\n",
    "    \"yolov7-w6-pose.pt\"\n",
    ")\n",
    "model = weigths['model']\n",
    "model = model.half().to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920\n",
      "1088\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"../smear-beta/\")\n",
    "video_path = local_config['PATH']['raw_videos']+\"vidal_test.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if (cap.isOpened() == False):\n",
    "    print('Video could not be read')\n",
    " \n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "\n",
    "scale_by = (\n",
    "    frame_width if frame_width > frame_height\n",
    "    else frame_height\n",
    ")\n",
    "scale_factor = 1\n",
    " \n",
    "# Pass the first frame through `letterbox` function to get the resized image,\n",
    "# to be used for `VideoWriter` dimensions. Resize by larger side.\n",
    "vid_write_image = letterbox(cap.read()[1], int(scale_by/scale_factor), stride=64, auto=True)[0]\n",
    "resize_height, resize_width = vid_write_image.shape[:2]\n",
    "\n",
    "print(resize_height)\n",
    "print(resize_width)\n",
    " \n",
    "save_name = local_config['PATH']['output_videos']+\"yolov7_vidal_test.mp4\"\n",
    "\n",
    "# Define codec and create VideoWriter object .\n",
    "out = cv2.VideoWriter(\n",
    "    save_name,\n",
    "    cv2.VideoWriter_fourcc(*'mp4v'), \n",
    "    30,\n",
    "    (resize_width, resize_height)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marce\\miniconda3\\envs\\tfg2\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     17\u001b[0m     output, _ \u001b[39m=\u001b[39m model(image)\n\u001b[1;32m---> 19\u001b[0m output \u001b[39m=\u001b[39m non_max_suppression_kpt(output, \u001b[39m0.25\u001b[39;49m, \u001b[39m0.65\u001b[39;49m, nc\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49myaml[\u001b[39m'\u001b[39;49m\u001b[39mnc\u001b[39;49m\u001b[39m'\u001b[39;49m], nkpt\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49myaml[\u001b[39m'\u001b[39;49m\u001b[39mnkpt\u001b[39;49m\u001b[39m'\u001b[39;49m], kpt_label\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     20\u001b[0m output \u001b[39m=\u001b[39m output_to_keypoint(output)\n\u001b[0;32m     21\u001b[0m nimg \u001b[39m=\u001b[39m image[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\marce\\Desktop\\_tfg\\yolov7\\utils\\general.py:728\u001b[0m, in \u001b[0;36mnon_max_suppression_kpt\u001b[1;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, kpt_label, nc, nkpt)\u001b[0m\n\u001b[0;32m    724\u001b[0m output \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mzeros((\u001b[39m0\u001b[39m,\u001b[39m6\u001b[39m), device\u001b[39m=\u001b[39mprediction\u001b[39m.\u001b[39mdevice)] \u001b[39m*\u001b[39m prediction\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    725\u001b[0m \u001b[39mfor\u001b[39;00m xi, x \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(prediction):  \u001b[39m# image index, image inference\u001b[39;00m\n\u001b[0;32m    726\u001b[0m     \u001b[39m# Apply constraints\u001b[39;00m\n\u001b[0;32m    727\u001b[0m     \u001b[39m# x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\u001b[39;00m\n\u001b[1;32m--> 728\u001b[0m     x \u001b[39m=\u001b[39m x[xc[xi]]  \u001b[39m# confidence\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     \u001b[39m# Cat apriori labels if autolabelling\u001b[39;00m\n\u001b[0;32m    731\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(labels[xi]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while(cap.isOpened):\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "  \n",
    "    if ret:\n",
    "        orig_image = frame\n",
    "        image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "        image = letterbox(image, int(scale_by/scale_factor), stride=64, auto=True)[0]\n",
    "        image_ = image.copy()\n",
    "        image = transforms.ToTensor()(image)\n",
    "        image = torch.tensor(np.array([image.numpy()]))\n",
    "        image = image.to(device)\n",
    "        image = image.half()\n",
    "    \n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, _ = model(image)\n",
    "    \n",
    "        output = non_max_suppression_kpt(output, 0.25, 0.65, nc=model.yaml['nc'], nkpt=model.yaml['nkpt'], kpt_label=True)\n",
    "        output = output_to_keypoint(output)\n",
    "        nimg = image[0].permute(1, 2, 0) * 255\n",
    "        nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "        nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "        for idx in range(output.shape[0]):\n",
    "            plot_skeleton_kpts(nimg, output[idx, 7:].T, 3)\n",
    "    \n",
    "            # Comment/Uncomment the following lines to show bounding boxes around persons.\n",
    "            # xmin, ymin = (output[idx, 2]-output[idx, 4]/2), (output[idx, 3]-output[idx, 5]/2)\n",
    "            # xmax, ymax = (output[idx, 2]+output[idx, 4]/2), (output[idx, 3]+output[idx, 5]/2)\n",
    "            # cv2.rectangle(\n",
    "            #     nimg,\n",
    "            #     (int(xmin), int(ymin)),\n",
    "            #     (int(xmax), int(ymax)),\n",
    "            #     color=(255, 0, 0),\n",
    "            #     thickness=1,\n",
    "            #     lineType=cv2.LINE_AA\n",
    "            # )\n",
    "    \n",
    "        # Write the FPS on the current frame.\n",
    "        # cv2.putText(nimg, f\"{fps:.3f} FPS\", (15, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        #             1, (0, 255, 0), 2)\n",
    "        # Convert from BGR to RGB color format.\n",
    "        # cv2.imshow('image', nimg)\n",
    "        out.write(nimg)\n",
    "        # Press `q` to exit.\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        # No more frames to read\n",
    "        break\n",
    "  \n",
    "\n",
    "# Release VideoCapture().\n",
    "cap.release()\n",
    "out.release()\n",
    "# Close all frames and video windows.\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
